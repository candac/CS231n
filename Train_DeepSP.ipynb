{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_DeepSP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "19kdtSOIcg51E_6zsDYJhdF1OSb7emEMm",
      "authorship_tag": "ABX9TyOC49fdjmKiwj+sn/uUt8Nw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/candac/CS231n/blob/master/Train_DeepSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOcB-7eNoRwR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import collections\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "\n",
        "import json\n",
        "\n",
        "torch.set_printoptions(precision=6)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGLbrwIcm1k9"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hodzyvG-omD2"
      },
      "source": [
        "class StayPointTrainDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        #load dataset\n",
        "        self.data_path = data_path\n",
        "        self.trainset = [json.loads(line) for line in open(self.data_path + 'sample_train.json', 'r')]\n",
        "        self.train_size = len(self.trainset)\n",
        "        mapslen = pd.read_csv(self.data_path + \"maplengths.csv\", names=[\"usermaplength\"], header=None)\n",
        "        self.num_userid = mapslen[\"usermaplength\"].values[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        #return length of dataset\n",
        "        return len(self.trainset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #return a sample of dataset\n",
        "        x_session, y_session, realtime_session = \\\n",
        "        self.session_processing(self.trainset[idx][\"stays\"], self.trainset[idx][\"num_stays\"])\n",
        "        return (x_session, y_session, realtime_session, int(self.trainset[idx][\"user_id\"][2:]))\n",
        "\n",
        "\n",
        "    def session_processing(self, session, numstays):  \n",
        "        x_session, y_session, realtime_session = np.zeros(numstays), np.zeros(numstays),np.zeros(numstays)\n",
        "        for i,chkin in enumerate(session):\n",
        "            x, y, real_time = chkin[\"x\"],chkin[\"y\"], chkin[\"time_index\"]\n",
        "            x_session[i] = np.float(x)\n",
        "            y_session[i] = np.float(y)\n",
        "            realtime_session[i] = real_time\n",
        "        return list(x_session), list(y_session), list(realtime_session)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXZUwXk8eUPA"
      },
      "source": [
        "class StayPointValidationDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        #load dataset\n",
        "        self.data_path = data_path\n",
        "        self.valset = [json.loads(line) for line in open(self.data_path + 'sample_val.json', 'r')]\n",
        "        self.val_size = len(self.valset)\n",
        "        mapslen = pd.read_csv(self.data_path + \"maplengths.csv\", names=[\"usermaplength\"], header=None)\n",
        "        self.num_userid = mapslen[\"usermaplength\"].values[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        #return length of dataset\n",
        "        return len(self.valset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #return a sample of dataset\n",
        "        x_session, y_session, realtime_session = \\\n",
        "        self.session_processing(self.valset[idx][\"stays\"], self.valset[idx][\"num_stays\"])\n",
        "        return (x_session, y_session, realtime_session, int(self.valset[idx][\"user_id\"][2:]))\n",
        "\n",
        "\n",
        "    def session_processing(self, session, numstays):  \n",
        "        x_session, y_session, realtime_session = np.zeros(numstays), np.zeros(numstays),np.zeros(numstays)\n",
        "        for i,chkin in enumerate(session):\n",
        "            x, y, real_time = chkin[\"x\"],chkin[\"y\"], chkin[\"time_index\"]\n",
        "            x_session[i] = np.float(x)\n",
        "            y_session[i] = np.float(y)\n",
        "            realtime_session[i] = real_time\n",
        "        return list(x_session), list(y_session), list(realtime_session)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27nW2vureXqR"
      },
      "source": [
        "def pad_collate(batch):\n",
        "    (xx, yy, tt, uu) = zip(*batch)\n",
        "    uu_ = torch.LongTensor(uu)\n",
        "    seq_lens = torch.LongTensor([len(x) for x in xx]) # same value for yy and tt\n",
        "\n",
        "    batch_x = [ torch.FloatTensor(x) for x in xx ]\n",
        "    batch_y = [ torch.FloatTensor(y) for y in yy ]\n",
        "    batch_t = [ torch.LongTensor(t) for t in tt ]\n",
        "\n",
        "    xx_pad = pad_sequence(batch_x, batch_first=True, padding_value=0.0)\n",
        "    yy_pad = pad_sequence(batch_y, batch_first=True, padding_value=0.0)\n",
        "    tt_pad = pad_sequence(batch_t, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    #return xx_pad, yy_pad, tt_pad, seq_lens, mask\n",
        "    return xx_pad.to(device), yy_pad.to(device), tt_pad.to(device), uu_.to(device), seq_lens.to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXONbB_pVzT"
      },
      "source": [
        "## Bidirectional RNN-GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvFIFhJSon4P"
      },
      "source": [
        "class Flat_RNN_GRU(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_layers, dropout, bidirectional):\n",
        "        super(Flat_RNN_GRU, self).__init__()\n",
        "        #Network layer sizes\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = 2\n",
        "        \n",
        "        #Output layers\n",
        "        self.gru = nn.GRU(3, hidden_dim, batch_first=True, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        #self.fc_final = nn.Linear(hidden_dim, self.output_dim)  # uni-directional RNN\n",
        "        self.fc_final = nn.Linear(hidden_dim*2, self.output_dim) # bi-directional RNN\n",
        "\n",
        "    def forward(self, inputs_x, inputs_y, inputs_time, inputs_user, session_lengths):\n",
        "        #input_x to same format as time_embed and user_embed\n",
        "        inputs_x = inputs_x.view(inputs_x.size()[0],inputs_x.size()[1], 1)\n",
        "        #input_y to same format as time_embed and user_embed\n",
        "        inputs_y = inputs_y.view(inputs_y.size()[0],inputs_y.size()[1], 1)\n",
        "        #input_t to same format \n",
        "        inputs_t = inputs_time.view(inputs_time.size()[0],inputs_time.size()[1], 1)\n",
        "\n",
        "\n",
        "        x = torch.cat(( inputs_x, inputs_y, inputs_t ), dim=2) #on tensor dimension 2 concatenate\n",
        "\n",
        "        gru_output, _ = self.gru(x) #to do: add init hidden\n",
        "\n",
        "        final_output = self.fc_final(gru_output) #to do: is fc_final needed for just gru_output\n",
        "\n",
        "        idxs = session_lengths - 2\n",
        "        hidden_indices = idxs.view(-1, 1, 1).expand(gru_output.size(0), 1, gru_output.size(2))\n",
        "        hidden_out = torch.gather(gru_output, 1, hidden_indices)\n",
        "        hidden_out = hidden_out.squeeze().unsqueeze(0) \n",
        "        return final_output, gru_output, hidden_out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu5EBO3aot1C"
      },
      "source": [
        "def masked_squared_error_loss(y_hat, y):\n",
        "    \"\"\"\n",
        "    y_hat:  batch*sequence_len x 2\n",
        "    y:  batch*sequence_len x 2\n",
        "    --------------\n",
        "    loss: distance(y_hat, y) **2\n",
        "    \"\"\"\n",
        "    lat_hat = torch.transpose(y_hat,0,1)[0] + 1\n",
        "    lon_hat = torch.transpose(y_hat,0,1)[1] + 103\n",
        "    #y_hat2 = torch.cat([lat_hat.view(-1,1), lon_hat.view(-1,1)],1)\n",
        "\n",
        "    lat = torch.transpose(y,0,1)[0] + 1\n",
        "    lon = torch.transpose(y,0,1)[1] + 103\n",
        "    #y2 = torch.cat([lat.view(-1,1), lon.view(-1,1)],1)\n",
        "\n",
        "    dist = tensor_distance(lat_hat, lon_hat, lat, lon)\n",
        "    dist2 = dist**2\n",
        "\n",
        "    mask = torch.transpose(y.sign().float(),0,1)[0] # to filter out padding\n",
        "    loss = dist2 * mask   # makes zeros where padding was added\n",
        "\n",
        "    return loss.view(-1, 1)  #torch.Size([batch * sequence_len x 1])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q6cFGj_JRR7"
      },
      "source": [
        "def tensor_distance(lat_hat, lon_hat, lat, lon): \n",
        "    \"\"\"\n",
        "    Calculate the equirectangular approx distance between two points\n",
        "    on the earth (specified in decimal degrees)\n",
        "    \"\"\"\n",
        "    # transform from degrees to radians\n",
        "    lat2 = torch.deg2rad(lat_hat)\n",
        "    lat1 = torch.deg2rad(lat)\n",
        "    lon2 = torch.deg2rad(lon_hat)\n",
        "    lon1 = torch.deg2rad(lon)\n",
        "\n",
        "    # equirectangular approximation\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    x = (lon2 - lon1) * torch.cos((lat1 + lat2)/2.)\n",
        "    y = (lat2 - lat1) \n",
        "    r = 6371  # Earth's radius in km\n",
        "    return torch.sqrt(x**2 + y**2) * r"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMMe87f5o_9R"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyqdVHieo4L0"
      },
      "source": [
        "def process_train(x_batch, y_batch, t_batch):\n",
        "    return x_batch[:, :-1], x_batch[:,1:], y_batch[:, :-1], y_batch[:,1:], t_batch[:, :-1].to(torch.int64), t_batch[:,1:].to(torch.int64)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSHjRaq2pCU4"
      },
      "source": [
        "## Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FVAlTNYszqF",
        "outputId": "a71b2002-6954-474a-9700-6ebee1bf79ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 64 #32, 64\n",
        "hidden_size = 120 #150\n",
        "learning_rate = 0.0005 #0.0005, 0.0001\n",
        "num_layers = 2 #2\n",
        "max_grad_norm = 1.0\n",
        "dropout= 0.15    #0.15, 0.05\n",
        "bidirectional = True\n",
        "\n",
        "#Init\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_path = \"/content/drive/My Drive/DeepSP/data/medium_sample/\"\n",
        "train_dataset = StayPointTrainDataset(data_path)\n",
        "trainloader = DataLoader(dataset=train_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=pad_collate, # use custom collate function here\n",
        "                      pin_memory=False)\n",
        "print(\"training dataset loaded\")\n",
        "\n",
        "val_dataset = StayPointValidationDataset(data_path)\n",
        "valloader = DataLoader(dataset=val_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=pad_collate, # use custom collate function here\n",
        "                      pin_memory=False)\n",
        "print(\"validation dataset loaded\")\n",
        "\n",
        "intra_rnn_model = Flat_RNN_GRU(hidden_size, num_layers, dropout, bidirectional).to(device) #Simple model\n",
        "\n",
        "pred_optim = torch.optim.Adam(intra_rnn_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# #Uncomment to load model; if training from scratch comment out\n",
        "# PATH = \"/content/drive/My Drive/DeepSP/saved\" + '/model_best_20201004'\n",
        "# checkpoint = torch.load(PATH) #Attention with map_location=torch.device('cpu')\n",
        "# intra_rnn_model.load_state_dict(checkpoint['intra_rnn'])\n",
        "# pred_optim.load_state_dict(checkpoint['pred_optim'])\n",
        "# print(\"model loaded\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training dataset loaded\n",
            "validation dataset loaded\n",
            "model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD6anFRXsMGR"
      },
      "source": [
        "# epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIARQ9vOgAJb",
        "outputId": "24e9f55e-2c7b-48d5-c01e-b63553a92377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Epoch loop\n",
        "for epoch in range(epochs):\n",
        "    intra_rnn_model.train()\n",
        "    loss_epoch = 0\n",
        "    vloss_epoch = 0\n",
        "    last_vloss_epoch = 0\n",
        "    count = 0\n",
        "    list_last_predict = []\n",
        "    list_last_target = []\n",
        "    list_second_last = []\n",
        "\n",
        "    #Loop for batches\n",
        "    for i_batch, (xx_pad, yy_pad, tt_pad, batch_user, seq_lens) in enumerate(trainloader):\n",
        "\n",
        "        #Divide into inputs and targets\n",
        "        x_pad, x_pad_target, y_pad, y_pad_target, t_pad, t_pad_target = process_train(xx_pad, yy_pad, tt_pad)\n",
        "\n",
        "        #RNN\n",
        "        predicted, _, _ = intra_rnn_model(x_pad, y_pad, t_pad, batch_user, seq_lens)\n",
        "        \n",
        "        #Loss\n",
        "        predicted_loss = masked_squared_error_loss(predicted.view(-1, 2), \\\n",
        "                                                   torch.cat([x_pad_target.reshape(-1,1), \\\n",
        "                                                              y_pad_target.reshape(-1,1)],dim=1))\n",
        "        \n",
        "        mean_loss = predicted_loss.sum(0) / (seq_lens - 1).sum().float()\n",
        "\n",
        "        pred_optim.zero_grad() # set gradients to zero\n",
        "        mean_loss.backward()\n",
        "        clip_grad_norm_(intra_rnn_model.parameters(), max_grad_norm)\n",
        "        pred_optim.step() # update weigths\n",
        "\n",
        "\n",
        "        loss_epoch += float(mean_loss.item())\n",
        "\n",
        "\n",
        "        if (i_batch + 1) % 200 == 0:\n",
        "            print('Epoch [{}/{}], Step {}, Left user: {}, Loss: {:.4f}'\n",
        "                  .format(epoch+1, epochs, i_batch+1, train_dataset.train_size - ((i_batch+1) * batch_size),\n",
        "                          mean_loss.item()))\n",
        "    \n",
        "\n",
        "    #Validation\n",
        "    intra_rnn_model.eval()\n",
        "\n",
        "    #Loop for batches\n",
        "    for v_batch, (xx_pad, yy_pad, tt_pad, batch_user, seq_lens) in enumerate(valloader):\n",
        "\n",
        "        #Divide into inputs and targets\n",
        "        x_pad, x_pad_target, y_pad, y_pad_target, t_pad, t_pad_target = process_train(xx_pad, yy_pad, tt_pad)\n",
        "\n",
        "        #RNN\n",
        "        vpredicted, _, _ = intra_rnn_model(x_pad, y_pad, t_pad, batch_user, seq_lens)\n",
        "        \n",
        "        #Loss\n",
        "        vpredicted_loss = masked_squared_error_loss(vpredicted.view(-1, 2), \\\n",
        "                                                   torch.cat([x_pad_target.reshape(-1,1), \\\n",
        "                                                              y_pad_target.reshape(-1,1)],dim=1))\n",
        "        \n",
        "        vmean_loss = vpredicted_loss.sum(0) / (seq_lens - 1).sum().float()\n",
        "        vloss_epoch += float(vmean_loss.item())\n",
        "\n",
        "        #Last staypoint loss\n",
        "        idxs = seq_lens - 2\n",
        "\n",
        "        target_last_idxs = idxs.view(-1, 1)\n",
        "        x_pad_target_last = torch.gather(x_pad_target, 1, target_last_idxs)\n",
        "        y_pad_target_last = torch.gather(y_pad_target, 1, target_last_idxs)\n",
        "        target_last = torch.cat([x_pad_target_last, y_pad_target_last], dim=1)\n",
        "\n",
        "        predicted_last_idxs = idxs.view(-1, 1)\\\n",
        "        .expand(vpredicted.size()[0],vpredicted.size()[2])\\\n",
        "        .reshape(vpredicted.size()[0], 1, vpredicted.size()[2])\n",
        "        vpredicted_last = torch.gather(vpredicted, 1, predicted_last_idxs)\\\n",
        "        .reshape(vpredicted.size()[0], vpredicted.size()[2])\n",
        "\n",
        "        last_vpredicted_loss = masked_squared_error_loss(vpredicted_last, target_last)\n",
        "        last_vmean_loss = last_vpredicted_loss.sum(0) / len(seq_lens)\n",
        "        last_vloss_epoch += float(last_vmean_loss.item())\n",
        "\n",
        "        #target second last (i.e. last observable stay-point)\n",
        "        x_second_last = torch.gather(x_pad, 1, target_last_idxs)\n",
        "        y_second_last = torch.gather(y_pad, 1, target_last_idxs)\n",
        "        second_last = torch.cat([x_second_last, y_second_last], dim=1)\n",
        "\n",
        "\n",
        "        #Save if last batch\n",
        "        if epoch == (epochs - 1):\n",
        "            list_last_predict.append(vpredicted_last.detach())\n",
        "            list_last_target.append(target_last.detach())\n",
        "            list_second_last.append(second_last.detach())\n",
        "    \n",
        "    #Print epoch results\n",
        "    print(\"Epoch {} finished, average train loss of this epoch is {:.4f}, \\\n",
        "    average val loss of this epoch is {:.4f}, average last staypoint loss of this epoch is {:.4f}\"\n",
        "          .format(epoch + 1, loss_epoch / (i_batch+1), vloss_epoch / (v_batch+1), last_vloss_epoch/(v_batch+1)))  \n",
        "    \n",
        "    # #Uncomment to create saving checkpoints for network\n",
        "    # if (epoch + 1) % (10) == 0:\n",
        "    #     torch.save({\"intra_rnn\": intra_rnn_model.state_dict(),\n",
        "    #                 \"pred_optim\": pred_optim.state_dict()}, \"/content/drive/My Drive/DeepSP/saved\" + '/model_epoch{}'.format(epoch + 1))\n",
        "    #     print(\"Epoch {} model saved\".format(epoch + 1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step 200, Left user: 7811, Loss: 5.9655\n",
            "Epoch 1 finished, average train loss of this epoch is 8.6839,     average val loss of this epoch is 2.1329, average last staypoint loss of this epoch is 5.2492\n",
            "Epoch [2/10], Step 200, Left user: 7811, Loss: 4.9720\n",
            "Epoch 2 finished, average train loss of this epoch is 8.3440,     average val loss of this epoch is 11.8856, average last staypoint loss of this epoch is 19.4549\n",
            "Epoch [3/10], Step 200, Left user: 7811, Loss: 4.3069\n",
            "Epoch 3 finished, average train loss of this epoch is 5.4102,     average val loss of this epoch is 2.3166, average last staypoint loss of this epoch is 5.4532\n",
            "Epoch [4/10], Step 200, Left user: 7811, Loss: 7.7807\n",
            "Epoch 4 finished, average train loss of this epoch is 11.0985,     average val loss of this epoch is 11.6289, average last staypoint loss of this epoch is 13.5865\n",
            "Epoch [5/10], Step 200, Left user: 7811, Loss: 5.6344\n",
            "Epoch 5 finished, average train loss of this epoch is 7.6922,     average val loss of this epoch is 3.6511, average last staypoint loss of this epoch is 7.5498\n",
            "Epoch [6/10], Step 200, Left user: 7811, Loss: 3.9094\n",
            "Epoch 6 finished, average train loss of this epoch is 4.4159,     average val loss of this epoch is 2.9482, average last staypoint loss of this epoch is 5.8515\n",
            "Epoch [7/10], Step 200, Left user: 7811, Loss: 4.2658\n",
            "Epoch 7 finished, average train loss of this epoch is 13.3330,     average val loss of this epoch is 3.8814, average last staypoint loss of this epoch is 7.4331\n",
            "Epoch [8/10], Step 200, Left user: 7811, Loss: 5.4744\n",
            "Epoch 8 finished, average train loss of this epoch is 7.4718,     average val loss of this epoch is 9.2953, average last staypoint loss of this epoch is 17.4336\n",
            "Epoch [9/10], Step 200, Left user: 7811, Loss: 4.2476\n",
            "Epoch 9 finished, average train loss of this epoch is 12.3586,     average val loss of this epoch is 1.6483, average last staypoint loss of this epoch is 4.0575\n",
            "Epoch [10/10], Step 200, Left user: 7811, Loss: 3.6614\n",
            "Epoch 10 finished, average train loss of this epoch is 4.4267,     average val loss of this epoch is 2.7272, average last staypoint loss of this epoch is 6.6190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCiiKviaQyg4"
      },
      "source": [
        "## Runs log - Medium Sample\n",
        "\n",
        "###Bidirectional Simple Model\n",
        "\n",
        "Epoch 50, average test loss is 20.5\n",
        "\n",
        "Epoch 100, average test loss is 4.7683\n",
        "\n",
        "Epoch >100, average test lost is 1.6483\n",
        "\n",
        "#### Parameters\n",
        "<ul>\n",
        "<li>epochs = >100</li>\n",
        "<li>batch_size = 64 #32, 64 </li>\n",
        "<li>hidden_size = 120 #150</li>\n",
        "<li>learning_rate = 0.0005 #0.0005, 0.0001</li>\n",
        "<li>num_layers = 2 #2</li>\n",
        "<li>max_grad_norm = 1.0</li>\n",
        "<li>dropout=0.15    #0.15</li>\n",
        "<li>bidirectional = True</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSH4V7UnRCAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}